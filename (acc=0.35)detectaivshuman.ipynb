{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205},{"sourceId":10950703,"sourceType":"datasetVersion","datasetId":6811679},{"sourceId":10950867,"sourceType":"datasetVersion","datasetId":6811815}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Install ViT","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T01:16:14.591669Z","iopub.execute_input":"2025-03-07T01:16:14.591950Z","iopub.status.idle":"2025-03-07T01:16:18.953695Z","shell.execute_reply.started":"2025-03-07T01:16:14.591919Z","shell.execute_reply":"2025-03-07T01:16:18.952765Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom transformers import ViTForImageClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:49:45.220906Z","iopub.execute_input":"2025-03-12T01:49:45.221258Z","iopub.status.idle":"2025-03-12T01:49:45.225868Z","shell.execute_reply.started":"2025-03-12T01:49:45.221232Z","shell.execute_reply":"2025-03-12T01:49:45.225020Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Device Check","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:51:03.694347Z","iopub.execute_input":"2025-03-12T01:51:03.694664Z","iopub.status.idle":"2025-03-12T01:51:03.699381Z","shell.execute_reply.started":"2025-03-12T01:51:03.694640Z","shell.execute_reply":"2025-03-12T01:51:03.698693Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\",\n    num_labels = 2,\n    ignore_mismatched_sizes=True\n)\nmodel.to(device)\nprint(\"viT Model Loaded Successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:55:18.081273Z","iopub.execute_input":"2025-03-12T01:55:18.081601Z","iopub.status.idle":"2025-03-12T01:55:19.182911Z","shell.execute_reply.started":"2025-03-12T01:55:18.081569Z","shell.execute_reply":"2025-03-12T01:55:19.182138Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"viT Model Loaded Successfully\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### Unlock Some Layer of Model","metadata":{}},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\nfor param in model.vit.encoder.layer[-4:].parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:59:03.149292Z","iopub.execute_input":"2025-03-12T01:59:03.149640Z","iopub.status.idle":"2025-03-12T01:59:03.154638Z","shell.execute_reply.started":"2025-03-12T01:59:03.149615Z","shell.execute_reply":"2025-03-12T01:59:03.153716Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Load Information and Create DataLodaer","metadata":{}},{"cell_type":"code","source":"# Path of Dataset\nBASE_PATH = \"/kaggle/input/ai-vs-human-generated-dataset\"\nTRAIN_CSV = os.path.join(BASE_PATH, \"train.csv\")\nTEST_CSV = os.path.join(BASE_PATH, \"test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:59:15.613900Z","iopub.execute_input":"2025-03-12T01:59:15.614180Z","iopub.status.idle":"2025-03-12T01:59:15.618003Z","shell.execute_reply.started":"2025-03-12T01:59:15.614157Z","shell.execute_reply":"2025-03-12T01:59:15.617213Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Load CSV\ndf_train = pd.read_csv(os.path.join(BASE_PATH, \"train.csv\"))\ndf_test = pd.read_csv(os.path.join(BASE_PATH, \"test.csv\"))\n\nprint(\"train.csv:\")\nprint(df_train.head())\n\nprint(\"\\n test.csv:\")\nprint(df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:59:29.621079Z","iopub.execute_input":"2025-03-12T01:59:29.621374Z","iopub.status.idle":"2025-03-12T01:59:29.712577Z","shell.execute_reply.started":"2025-03-12T01:59:29.621352Z","shell.execute_reply":"2025-03-12T01:59:29.711693Z"}},"outputs":[{"name":"stdout","text":"train.csv:\n   Unnamed: 0                                        file_name  label\n0           0  train_data/a6dcb93f596a43249135678dfcfc17ea.jpg      1\n1           1  train_data/041be3153810433ab146bc97d5af505c.jpg      0\n2           2  train_data/615df26ce9494e5db2f70e57ce7a3a4f.jpg      1\n3           3  train_data/8542fe161d9147be8e835e50c0de39cd.jpg      0\n4           4  train_data/5d81fa12bc3b4cea8c94a6700a477cf2.jpg      1\n\n test.csv:\n                                                  id\n0  test_data_v2/1a2d9fd3e21b4266aea1f66b30aed157.jpg\n1  test_data_v2/ab5df8f441fe4fbf9dc9c6baae699dc7.jpg\n2  test_data_v2/eb364dd2dfe34feda0e52466b7ce7956.jpg\n3  test_data_v2/f76c2580e9644d85a741a42c6f6b39c0.jpg\n4  test_data_v2/a16495c578b7494683805484ca27cf9f.jpg\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"df_train[\"file_path\"] = df_train[\"file_name\"].apply(lambda x: os.path.join(BASE_PATH, x))\ndf_test[\"file_path\"] = df_test[\"id\"].apply(lambda x: os.path.join(BASE_PATH, x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:59:31.646859Z","iopub.execute_input":"2025-03-12T01:59:31.647150Z","iopub.status.idle":"2025-03-12T01:59:31.744927Z","shell.execute_reply.started":"2025-03-12T01:59:31.647129Z","shell.execute_reply":"2025-03-12T01:59:31.744274Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(df_train[\"file_name\"].head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:59:33.165993Z","iopub.execute_input":"2025-03-12T01:59:33.166273Z","iopub.status.idle":"2025-03-12T01:59:33.171652Z","shell.execute_reply.started":"2025-03-12T01:59:33.166252Z","shell.execute_reply":"2025-03-12T01:59:33.170628Z"}},"outputs":[{"name":"stdout","text":"0    train_data/a6dcb93f596a43249135678dfcfc17ea.jpg\n1    train_data/041be3153810433ab146bc97d5af505c.jpg\n2    train_data/615df26ce9494e5db2f70e57ce7a3a4f.jpg\n3    train_data/8542fe161d9147be8e835e50c0de39cd.jpg\n4    train_data/5d81fa12bc3b4cea8c94a6700a477cf2.jpg\n5    train_data/25ea852f30594bc5915eb929682af429.jpg\n6    train_data/e67085fb6d814cbabe08f978c738f3f7.jpg\n7    train_data/041c36d9269146cdb88e7526e3b91651.jpg\n8    train_data/4aea3b876247467c8d3713d4920148ab.jpg\n9    train_data/09708379751e44d0bc908d8652d0db3e.jpg\nName: file_name, dtype: object\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"existing_files = df_train[\"file_path\"].apply(os.path.exists).sum()\nprint(f\"Find {existing_files} From {len(df_train)} in train_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T01:59:55.777367Z","iopub.execute_input":"2025-03-12T01:59:55.777723Z","iopub.status.idle":"2025-03-12T02:03:48.822975Z","shell.execute_reply.started":"2025-03-12T01:59:55.777693Z","shell.execute_reply":"2025-03-12T02:03:48.822159Z"}},"outputs":[{"name":"stdout","text":"Find 79950 From 79950 in train_data\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:03:55.773061Z","iopub.execute_input":"2025-03-12T02:03:55.773345Z","iopub.status.idle":"2025-03-12T02:03:55.778125Z","shell.execute_reply.started":"2025-03-12T02:03:55.773322Z","shell.execute_reply":"2025-03-12T02:03:55.777290Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Create Dataset","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, df, transform, test_mode=False):\n        self.df = df\n        self.transform = transform\n        self.test_mode = test_mode\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx][\"file_path\"]\n        img = Image.open(img_path).convert(\"RGB\")\n        img = self.transform(img)\n\n        if self.test_mode:\n            return img\n        else:\n            label = int(self.df.iloc[idx][\"label\"])\n            return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:12:18.911984Z","iopub.execute_input":"2025-03-12T02:12:18.912279Z","iopub.status.idle":"2025-03-12T02:12:18.917586Z","shell.execute_reply.started":"2025-03-12T02:12:18.912257Z","shell.execute_reply":"2025-03-12T02:12:18.916602Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### Split Train/Validation (80/20)","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42, stratify=df_train[\"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:12:26.560777Z","iopub.execute_input":"2025-03-12T02:12:26.561098Z","iopub.status.idle":"2025-03-12T02:12:26.597147Z","shell.execute_reply.started":"2025-03-12T02:12:26.561070Z","shell.execute_reply":"2025-03-12T02:12:26.596316Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### Create Dataloader","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16\ntrain_loader = DataLoader(ImageDataset(train_df, transform), batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(ImageDataset(val_df, transform), batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Train Data: {len(train_loader.dataset)} images\")\nprint(f\"Validation Data: {len(val_loader.dataset)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:12:28.092299Z","iopub.execute_input":"2025-03-12T02:12:28.092635Z","iopub.status.idle":"2025-03-12T02:12:28.098274Z","shell.execute_reply.started":"2025-03-12T02:12:28.092604Z","shell.execute_reply":"2025-03-12T02:12:28.097456Z"}},"outputs":[{"name":"stdout","text":"Train Data: 63960 images\nValidation Data: 15990 images\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Use AdamW Optimizer and CrossEntropy Loss\noptimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-4)\n\n# Use Label Smoothing reduce Overfitting\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, preds, target):\n        log_preds = torch.nn.functional.log_softmax(preds, dim=-1)\n        n_classes = preds.size(-1)\n        one_hot = torch.zeros_like(preds).scatter(1, target.unsqueeze(1), 1)\n        smoothed_labels = (1 - self.smoothing) * one_hot + self.smoothing / n_classes\n        return torch.mean(torch.sum(-smoothed_labels * log_preds, dim=-1))\n\nloss_fn = nn.CrossEntropyLoss()\n\n# Learning Rate Scheduler\nlr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, min_lr=1e-6, verbose=True)\n\n# Early Stopping\nearly_stopping_patience = 3\nbest_val_acc = 0\nearly_stop_counter = 0\n\n# Train Function\ndef train_epoch(model, train_loader):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs).logits\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        total_correct += (outputs.argmax(1) == labels).sum().item()\n    return total_loss / len(train_loader), total_correct / len(train_loader.dataset)\n\n# Validation Function\ndef evaluate(model, val_loader):\n    model.eval()\n    total_correct = 0\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs).logits\n            total_correct += (outputs.argmax(1) == labels).sum().item()\n    return total_correct / len(val_loader.dataset)\n\n# Model Train\nEPOCHS = 50\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = train_epoch(model, train_loader)\n    val_acc = evaluate(model, val_loader)\n\n    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n\n    lr_scheduler.step(val_acc)\n\nif val_acc > best_val_acc:\n    best_val_acc = val_acc\n    early_stop_counter = 0\n    torch.save(model.state_dict(), \"best_model.pth\") \nelse:\n    early_stop_counter += 1\n    if early_stop_counter >= early_stopping_patience:\n        print(f\" Early Stopping Triggered at Epoch {epoch+1}\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T01:19:32.116854Z","iopub.execute_input":"2025-03-07T01:19:32.117144Z","iopub.status.idle":"2025-03-07T08:09:29.622717Z","shell.execute_reply.started":"2025-03-07T01:19:32.117122Z","shell.execute_reply":"2025-03-07T08:09:29.621920Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50 | Train Loss: 0.0634 | Train Acc: 0.9768 | Val Acc: 0.9906\nEpoch 2/50 | Train Loss: 0.0307 | Train Acc: 0.9893 | Val Acc: 0.9894\nEpoch 3/50 | Train Loss: 0.0228 | Train Acc: 0.9921 | Val Acc: 0.9891\nEpoch 4/50 | Train Loss: 0.0201 | Train Acc: 0.9932 | Val Acc: 0.9923\nEpoch 5/50 | Train Loss: 0.0175 | Train Acc: 0.9942 | Val Acc: 0.9871\nEpoch 6/50 | Train Loss: 0.0165 | Train Acc: 0.9945 | Val Acc: 0.9931\nEpoch 7/50 | Train Loss: 0.0146 | Train Acc: 0.9949 | Val Acc: 0.9904\nEpoch 8/50 | Train Loss: 0.0137 | Train Acc: 0.9954 | Val Acc: 0.9898\nEpoch 9/50 | Train Loss: 0.0142 | Train Acc: 0.9952 | Val Acc: 0.9941\nEpoch 10/50 | Train Loss: 0.0133 | Train Acc: 0.9955 | Val Acc: 0.9921\nEpoch 11/50 | Train Loss: 0.0123 | Train Acc: 0.9960 | Val Acc: 0.9789\nEpoch 12/50 | Train Loss: 0.0134 | Train Acc: 0.9954 | Val Acc: 0.9904\nEarly Stopping Triggered at Epoch 12\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"MODEL_PATH = \"best_model.pth\"\ntorch.save(model.state_dict(), MODEL_PATH)\nprint(f\"Model saved successfully at {MODEL_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:03:00.084387Z","iopub.execute_input":"2025-03-07T09:03:00.084734Z","iopub.status.idle":"2025-03-07T09:03:00.591641Z","shell.execute_reply.started":"2025-03-07T09:03:00.084707Z","shell.execute_reply":"2025-03-07T09:03:00.590679Z"}},"outputs":[{"name":"stdout","text":"Model saved successfully at best_model.pth\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"### Load Best Model","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = \"best_model.pth\"\n\nif os.path.exists(MODEL_PATH):\n    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n    model.to(device)\n    print(\"Best Model Loaded Successfully!\")\nelse:\n    print(f\"Model file NOT found: {MODEL_PATH}. You may need to retrain and save the model.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:03:23.628854Z","iopub.execute_input":"2025-03-07T09:03:23.629221Z","iopub.status.idle":"2025-03-07T09:03:23.905128Z","shell.execute_reply.started":"2025-03-07T09:03:23.629187Z","shell.execute_reply":"2025-03-07T09:03:23.904350Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-36-a5c40bac2f9b>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Best Model Loaded Successfully!\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\n# Evaluate\ndef evaluate(model, val_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs).logits\n            preds = outputs.argmax(1).cpu().numpy()\n            labels = labels.cpu().numpy()\n            \n            all_preds.extend(preds)\n            all_labels.extend(labels)\n    \n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n    \n    print(f\"Validation Accuracy: {acc:.4f}\")\n    print(f\"Validation F1 Score: {f1:.4f}\")\n    \n    return acc, f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_acc, val_f1 = evaluate(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:04:31.311963Z","iopub.execute_input":"2025-03-07T09:04:31.312295Z","iopub.status.idle":"2025-03-07T09:08:14.374909Z","shell.execute_reply.started":"2025-03-07T09:04:31.312256Z","shell.execute_reply":"2025-03-07T09:08:14.373989Z"}},"outputs":[{"name":"stdout","text":"✅ Validation Accuracy: 0.9901\n✅ Validation F1 Score: 0.9901\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"LOG_FILE = \"training_log.txt\"\nlog_text = f\"Final Model Evaluation | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\\n\"\n\nwith open(LOG_FILE, \"w\") as f:\n    f.write(log_text)\n\nprint(f\"Training log saved at {LOG_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:08:43.876308Z","iopub.execute_input":"2025-03-07T09:08:43.876600Z","iopub.status.idle":"2025-03-07T09:08:43.881911Z","shell.execute_reply.started":"2025-03-07T09:08:43.876577Z","shell.execute_reply":"2025-03-07T09:08:43.881174Z"}},"outputs":[{"name":"stdout","text":"✅ Training log saved at training_log.txt\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Edit Dataset test set no label\nclass ImageDataset(Dataset):\n    def __init__(self, df, transform, test_mode=False):\n        self.df = df\n        self.transform = transform\n        self.test_mode = test_mode \n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx][\"file_path\"]\n        img = Image.open(img_path).convert(\"RGB\")\n        img = self.transform(img)\n\n        if self.test_mode:\n            return img\n        else:\n            label = int(self.df.iloc[idx][\"label\"])\n            return img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:15:03.670553Z","iopub.execute_input":"2025-03-07T09:15:03.670905Z","iopub.status.idle":"2025-03-07T09:15:03.676319Z","shell.execute_reply.started":"2025-03-07T09:15:03.670877Z","shell.execute_reply":"2025-03-07T09:15:03.675344Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def predict_test(model, test_df, transform, batch_size=8):\n    model.eval()\n    test_dataset = ImageDataset(test_df, transform, test_mode=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    predictions = []\n    ids = test_df[\"id\"].tolist()\n\n    with torch.no_grad():\n        for imgs in test_loader:\n            imgs = imgs.to(device)\n            outputs = model(imgs).logits\n            preds = outputs.argmax(1).cpu().numpy()\n            predictions.extend(preds)\n\n    return ids, predictions\n\ntest_ids, test_preds = predict_test(model, df_test, transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:11:31.520807Z","iopub.execute_input":"2025-03-12T02:11:31.521134Z","iopub.status.idle":"2025-03-12T02:11:31.526046Z","shell.execute_reply.started":"2025-03-12T02:11:31.521111Z","shell.execute_reply":"2025-03-12T02:11:31.525245Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"submission_df = pd.DataFrame({\"id\": test_ids, \"label\": test_preds})\nsubmission_df.to_csv(\"submission2.csv\", index=False)\n\nprint(f\"Submission file 'submission2.csv' created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:17:59.931533Z","iopub.execute_input":"2025-03-07T09:17:59.931837Z","iopub.status.idle":"2025-03-07T09:17:59.958269Z","shell.execute_reply.started":"2025-03-07T09:17:59.931802Z","shell.execute_reply":"2025-03-07T09:17:59.957449Z"}},"outputs":[{"name":"stdout","text":"✅ Submission file 'submission2.csv' created successfully!\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"### Test","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport torch\nfrom scipy import stats\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:31:18.155677Z","iopub.execute_input":"2025-03-07T09:31:18.156007Z","iopub.status.idle":"2025-03-07T09:31:18.159610Z","shell.execute_reply.started":"2025-03-07T09:31:18.155978Z","shell.execute_reply":"2025-03-07T09:31:18.158726Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def perform_ttest(ai_scores, human_scores):\n\n    # Check if there is enough data for T-Test\n    if len(ai_scores) < 2 or len(human_scores) < 2:\n        print(\"Information not enough for T-Test (need at least 2 samples per group).\")\n        return\n\n    # Calculate T-Test\n    t_stat, p_value = stats.ttest_ind(ai_scores, human_scores, equal_var=False)\n\n    # Show results\n    print(\"\\nT-Test Result:\")\n    print(f\"T-Statistic: {t_stat:.4f}\")\n    print(f\"P-Value: {p_value:.4f}\")\n\n    # Interpretation\n    if p_value < 0.05:\n        print(\"There is a statistically significant difference between AI-generated images and human-created images.\")\n    else:\n        print(\"No statistically significant difference was found between AI-generated images and human-created images.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:37:47.223813Z","iopub.execute_input":"2025-03-07T09:37:47.224123Z","iopub.status.idle":"2025-03-07T09:37:47.228870Z","shell.execute_reply.started":"2025-03-07T09:37:47.224101Z","shell.execute_reply":"2025-03-07T09:37:47.227989Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def predict_image(model, image_path, transform):\n\n    model.eval()  # Set the model to evaluation mode\n    \n    # Load and transform the image\n    img = Image.open(image_path).convert(\"RGB\")\n    img = transform(img)\n    img = img.unsqueeze(0).to(device)  # Add batch dimension and move to device\n\n    with torch.no_grad():\n        output = model(img).logits\n        pred = output.argmax(1).item()  # 0 = Human-Created, 1 = AI-Generated\n\n    # Interpret the result\n    result = \"AI-Generated\" if pred == 1 else \"Human-Created\"\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:39:11.503218Z","iopub.execute_input":"2025-03-07T09:39:11.503568Z","iopub.status.idle":"2025-03-07T09:39:11.508405Z","shell.execute_reply.started":"2025-03-07T09:39:11.503542Z","shell.execute_reply":"2025-03-07T09:39:11.507512Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"ai_scores = []\nhuman_scores = []\n\nUPLOAD_FOLDER = \"/kaggle/input/datadata\"\n\nif os.path.isdir(UPLOAD_FOLDER):\n    image_files = [f for f in os.listdir(UPLOAD_FOLDER) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n\n    if not image_files:\n        print(\"No upload image\")\n    else:\n        print(f\"Found {len(image_files)} uploaded images\")\n\n        for img_file in image_files:\n            img_path = os.path.join(UPLOAD_FOLDER, img_file)\n            result = predict_image(model, img_path, transform)\n\n            print(f\"Image: {img_file} → Prediction: {result}\")\n\n            if result == \"AI-Generated\":\n                ai_scores.append(1)\n            else:\n                human_scores.append(0)\n\n        perform_ttest(ai_scores, human_scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:43:21.363966Z","iopub.execute_input":"2025-03-07T09:43:21.364333Z","iopub.status.idle":"2025-03-07T09:43:21.420215Z","shell.execute_reply.started":"2025-03-07T09:43:21.364301Z","shell.execute_reply":"2025-03-07T09:43:21.419291Z"}},"outputs":[{"name":"stdout","text":"Found 2 uploaded images\nImage: download (2).jpg → Prediction: Human-Created\nImage: download.jpg → Prediction: Human-Created\nInformation not enough for T-Test (need at least 2 samples per group).\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}